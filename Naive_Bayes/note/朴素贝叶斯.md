[toc]



# 朴素贝叶斯

## 贝叶斯决策论

贝叶斯定理
$$
(H|E)=\frac{P(H)P(E|H)}{P(E)}=\frac{P(H)P(E|H)}{P(H)P(E|H)+P(\urcorner{H})P(E|\urcorner{H})}
$$

$$
P(A,B)=P(B)P(A|B)=P(A)P(B|A)
$$

条件风险
$$
R(c_i|x)=\sum_{j=1}^{N}\lambda_{ij}P(C_j|x)
$$

$$
\lambda_{ij}是将一个真实标记为c_j的样本误分类为c_i所产生的损失
$$

贝叶斯判定准则：为最小化总体风险，只需在每个样本上选择能使条件风险最小的类别标记
$$
h^*(x)=arg\min_{c\in{\gamma}R}R(c|x)
$$
结合误判条件风险，最小化分类错误率的贝叶斯最优分类器为
$$
h^*(x)=arg\max_{c\in{\gamma}R}p(c|x)
$$
在对p进行变形
$$
p(c|x)=\frac{P(x,c)}{P(x)}=\frac{P(x|c)P(c)}{P(x)}
$$
P(c)为先验概率，P(x|c)是类条件概率（似然）

## 朴素贝叶斯分类器

属性条件独立性假设
$$
h_{nb}(x) = arg\max_{c\in{\gamma}}P(C)\prod_{i=1}^{d}P(x_i|c)
$$

$$
P(c)=\frac{|D_c|}{|D|},P(x_i|c)=\frac{|D_{c,x_i}|}{|D_c|}
$$

连续型变量考虑概率密度函数（正太分布），求平均值和方差，代入公式

### 拉普拉斯修正

为了避免其他属性携带的信息被训练集中未出现的属性值抹去，在估计概率值时要进行平滑
$$
\hat{P}(c) = \frac{|D_c|+1}{|D|+N},N:训练集D中可能的类别数
$$

$$
\hat{P}(x_i|c) = \frac{|D_c,x_i|+1}{|D_c|+N_i},N_i:第i个属性可能的取值数
$$

## 半朴素贝叶斯分类器

考虑一部分属性间的互相依赖信息，从而既不需要进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系

#### 独依赖估计（ODE）

假设每个属性在类别之外最多仅依赖于一个其他属性

##### 超父SPODE

假设所有属性都依赖同一个属性，通过交叉验证等选择方法确定超父

##### TAN（Tree Augmented naive Bayes)

在最大的带权生成树算法的基础上构建属性见依赖广西

1）计算任意两个属性之间的条件互信息

2）以属性为结点构建完全图，任意两个结点之间边的权重设为I（xi,xj | y)

3）构建此完全图的最大带权生成树，挑选根变量，将边置为有向

4）加入类别结点y，增加从y到每个属性的有向边

##### AODE(AVerage One-Dependent Estimator)

基于集成学习机制，更为强大的独依赖分类器。尝试将每个属性作为超父构建SPODE，将具有足够训练数据支撑的SPODE集成起来作为最终结果
$$
\hat{P}(c,x_i) = \frac{|D_c|+1}{|D|+N+N_i},N_i:第i个属性的可能取值数
$$

$$
\hat{P}(x_k|c,x_i) = \frac{|D_{c,x_i,x_j}|+1}{|D_{c,x_i}|+N_j}
$$

# EM算法

最大期望算法经过两个步骤交替进行计算

1.  第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；

2.  第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。



### 极大似然估计

在数理统计学中，似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性。

而极大似然就相当于最大可能的意思。最大似然估计是已经知道了结果，然后寻求使该结果出现的可能性最大的条件，以此作为估计值。

极大似然估计用一句话概括就是：知道结果，反推条件θ。

### 隐变量

一般的用Y表示观测到的随机变量的数据，Z表示隐随机变量的数据。于是Y和Z连在一起被称为完全数据，仅Y一个被称为不完全数据

**Jensen不等式**

  设f是定义域为实数的函数

-   如果对于所有的实数x，f(x)的二次导数>=0，那么f是凸函数。
-   当**x**是向量时，如果其hessian矩阵H是半正定的（H>=0），那么f是凸函数。
-   如果f(x)的二次导数>0或者H>=0，那么称f是严格凸函数。

如果f是凸函数，X是随机变量，那么：E[f(X)]>=f(E[X])，通俗的说法是函数的期望大于等于期望的函数。

特别地，如果f是严格凸函数，当且仅当P(X = EX) = 1，即X是常量时，上式取等号，即E[f(X)] = f(EX)。

**期望公式中的Lazy Statistician规则**

![image-20210717211859410](C:\Users\姮娥\AppData\Roaming\Typora\typora-user-images\image-20210717211859410.png)

