# 集成学习

## 个体与集成

集成学习，又称多分类器系统、基于委员会的学习

同质：集成包括同种类型的个体学习器

基学习器：同质集成中的个体学习器

异质：集成包括不同类型的个体学习器

组件学习器：异质集成中的个体学习器

## Boosting

### AdaBoost

 融合一些弱的分类器获得强分类器

核心流程：先用一部分特征训练一些较弱的分类器，然后再将这些较弱的分类器逐步提升为强的分类器

1.   初始化每个样本的权值分布：

$$
D_1=(\omega_{11},\omega_{12},...,\omega_{1N}),\omega_{1i}=\frac{1}{N}
$$

2.   假设我们要构建M个弱分类器，那么对$m=1,2,...M,$首先从训练集中按照分布$D_m$​进行采样，将采样获得的数据用于训练，得到基本分类器$G_m(x)$​,其次计算$G_m(x)$在训练集上的误差：

$$
e_m=p(G_m(x_i)\neq y_i)=\sum_{i=1}^N\omega_{mi}I(G_m(x_i)\neq y_i)
$$

当$G_m(x_i)\neq y_i$时$I=1$,否则$I=0$​，这里$e_m$表示的时用每个样本权重分布$\omega_{mi}$加权平均后的错误率

接着计算第m个弱分类器的加权系数
$$
\alpha_m=\frac{1}{2}\log\frac{1-e_m}{e_m}
$$
结论1：由于这是个二分类问题，所以加权后错误率$e_m<1/2$，因此根据上面的公式可以得出，$\alpha_m>0$

结论2：$e_m$越小，则$\alpha_m$​越大，换句话说，第m个弱分类器的加权错误率越小，意味着这个分类器越准确，那么这个分类器对最后识别结果应影响的权重就应该越大

3.   更新训练集的权值分布

![image-20210803214835344](http://tenjoutena.oss-cn-guangzhou.aliyuncs.com/img/image-20210803214835344.png)

 $y_i$​是第i个训练样本的真实标签​；$G_m(x_i)$是第m个分类器对第i个样本的分类结果

4.   首先构建基本的分类面

$$
f(x)=\sum_{m=1}^M\alpha_mG_m(x)
$$

然后根据f(x)可以得到最终分类器
$$
G(x)=sign(f(x))=sign[\sum_{m=1}^M\alpha_mG_m(x)]
$$
每个分类器的权重系数$\alpha_m$越大，那么它对最终集成的费雷其G(x)的影响也越大

```python
#回归问题
#决策树回归
from sklearn.tree import DecisionTreeRegressor
#Adaboost中的adaboost回归
from sklearn.ensemble import AdaBoostRegressor
#线性回归模型
from sklearn.linear_model import LinearRegression
#用于划分训练集与测试集
from sklearn.model_selection import train_test_split

#训练，n_estimator:弱学习器最大迭代次数
#loss：线性‘linear’, 平方‘square’和指数 ‘exponential’三种选择, 默认是线性
#base_estimator:弱分类学习器或者弱回归学习器
#learning_rate:每个弱学习器的权重缩减系数                                      
reg = AdaBoostRegressor(LinearRegression(), n_estimators=1000)
reg.fit(x_train, y_train)
pred = reg.predict(x_test)
print('训练集得分：', reg.score(x_train, y_train))
print('测试集得分：', reg.score(x_test, y_test))
print('均方误差：', metrics.mean_squared_error(y_test, pred))

#分类问题
from sklearn.datasets import load_iris
#决策树分类
from sklearn.tree import DecisionTreeClassifier
#混淆矩阵
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
#Adaboost分类
from sklearn.ensemble import AdaBoostClassifier

iris = load_iris()
iris_data = iris.data
iris_target = iris.target
#划分训练集测试集
x_train, x_test, y_train ,y_test = train_test_split(iris_data, iris_target)

#n_estimator:弱学习器最大迭代次数
#learning_rate:每个弱学习器的权重缩减系数 
#algorityhm:scikit-learn实现了两种Adaboost分类算法，SAMME和SAMME.R，默认SAMME.R
clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5), n_estimators=100)
clf.fit(x_train, y_train.astype('int'))
pred = clf.predict(x_test)
confusion_matrix = confusion_matrix(pred, y_test)
print("训练集分数：",clf.score(x_train,y_train))
print("验证集分数：",clf.score(x_test,y_test))
print(confusion_matrix)

#用matplotlib绘制混淆矩阵
#分类的类别
classes = list(set(pred))
#imshow()接收一张图像，只是画出该图，并不会立刻显示出来，cmap可以选颜色，plt.cm.Oranges/Reds/Greens
plt.imshow(confusion_matrix, cmap=plt.cm.Blues)
indices = range(len(confusion_matrix))
plt.xticks(indices, classes)
plt.yticks(indices, classes)
plt.colorbar()
plt.xlabel('guess')
plt.ylabel('fact')
for first_index in range(len(confusion_matrix)):
    for second_index in range(len(confusion_matrix[first_index])):
        plt.text(first_index, second_index, confusion_matrix[first_index][second_index])
plt.show()
```

## Bagging（BootstrapAggrehating）

​		是并行式集成学习方法最著名的代表，它基于自主采样法。给定包含m个样本的数据集，先随机去除一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中。我没课采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。

 ![image-20210804102357822](http://tenjoutena.oss-cn-guangzhou.aliyuncs.com/img/image-20210804102357822.png)

Bagging主要关注降低方差



## 随机森林

RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。

## 结合策略

### 平均法

*   简单平均法

$$
H(x)=\frac{1}{T}\sum_{i=1}^Th_i(x)
$$

*   加权平均法

$$
H(x)=\sum_{i=1}^T\omega_ih_i(x)
$$



### 投票法

*   绝对多数投票法

若某标记得票过半数，则预测为该标记，否则拒绝预测

*   相对多数投票法

预测为得票最多的标记，若同时有多个标记获得高票，则从中随机选取一个

*   加权投票法

    

### 学习法

#### Stacking

Stacking是学习法的典型代表，我没将个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器

Srtacking先从初始数据集训练出初级学习器，然后生成一个新数据集 用于训练次级学习器，在这个新的数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。

![image-20210804112503936](http://tenjoutena.oss-cn-guangzhou.aliyuncs.com/img/image-20210804112503936.png)

![image-20210804113133620](http://tenjoutena.oss-cn-guangzhou.aliyuncs.com/img/image-20210804113133620.png)