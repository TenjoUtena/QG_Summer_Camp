[toc]



# 决策树

​		分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶节点。内部结点表示一个特征或属性，叶节点表示一个类。

**路径上内部结点的特征对应着规则的条件，而叶节点的类对应着规则的结论**

if-then规则性质：互斥且完备

## 特征选择

### 信息增商

熵：表示随机变量不确定行的度量

设X是一个取有限个值的离散随机变量，其概率分布为
$$
P(X=x_i)=p_i, i=1,2,...,n
$$

$$
H(X)=-\sum_{i=1}^{n}p_ilogp_i(对数以2或e为底)
$$

（统计学习方法）
$$
Ent(D)=-\sum_{k=1}^{|\gamma|}p_klob_2p_k
$$
(|y|：样本总数， p_k第k类样本所占的比列)

（西瓜书）

### 条件熵

在已知随机变量X的条件下随机变量Y的不确定性
$$
H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X = x_i)
$$

### 信息增益

得知特征X的信息而使得类Y的信息的不确定性减少的程度

特征A对训练集D的信息增益g(D,A),定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差
$$
g(D,A)=H(D)-H(D|A)
$$

$$
Gain(D,a)=-\sum_{k=1}^{|\gamma|}p_klob_2p_k+\sum_{v=1}^V\frac{|D^v|}{|D|}\sum_{k=1}^{|\gamma|}p_klob_2p_k
$$

### 信息增益比

特征A对训练数据集D的信息增益比GR(D,A)定义为其信息增益g(D,A)与训练集D关于特征A的值的熵HA(D)之比
$$
G_R(D,A)=\frac{g(D,A)}{H_A(D)}
$$

$$
H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}lob_2\frac{|D_i|}{|D|}
$$

又成为增益率

### 基尼系数

假设由K个类，样本点属于第k类的概率为pk,则概率分布的基尼指数定义为
$$
Gini(p)=\sum_{k=1}^{K}p_k(1-p)k=a-\sum_{k=1}^{K}p_k^2
$$
二分类问题
$$
Gini(p)=2p(1-p)
$$
集合D的基尼指数定义为
$$
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$


## 决策树的生成

### ID3

(信息增益)

对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点

算法：

输入：训练数据集D，特征值A阈值epsilon

输出：决策树T

1）若D中所有实例属于同一类Ck,则T为单结点树，并将类Ck作为该结点的类标记，返回T

2）若A为空，则T为单结点树，并将D中实例树最大的类Ck作为该结点的类标记，返回T

3）否则，计算A中各特征对D的信息增益，选择信息增益最大的特征Ag

4）如果Ag的信息增益小于阈值，则置T为单结点树，并将D中实例树最大的类Ck作为该结点的类标记

5）否则，对Ag的每一可能值ai，依据Ag=ai将D分割为若干非空子集Di,将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成数T，返回T

6）对第i个子结点，以Di为训练集，以A-{Ag}为特征值，递归调用步1）~5）得到子树Ti,返回Ti



### C4.5

(信息增益比)

算法与ID3一致，将ID3的信息增益换位信息增益比



### CART

#### 回归树的生成

（平方误差最小化）

1)选择最优切分变量j与切分点s,求解：
$$
\min_{j,s}=[\min_{c1}\sum_{xi\in{R1(j,xs)}}(y_i-c_1)^2+\min_{c2}\sum_{xi\in{R2(j,xs)}}(y_i-c_2)^2]
$$
2)用选定的对(j,s)划分区域并决定相应的输出值

3）继续对两个子区域调用步骤1）2）

4）将输入空间划分为M个区域R1,R2,...RM,生成决策树
$$
f(x)=\sum_{m=1}^{M}\hat{c}_mI(x\in{R_m})
$$


#### 分类树的生成

（基尼系数）

算法：

1）设结点的训练数据集为D，计算现有特征对该数据集的基尼指数。此时对每一个特征A，对其可能取得每个值a，根据样本点对A=a得测试为“是”或“否”将D分割成D1,D2两部分，计算A=a时得基尼指数。

2）在所有可能得特征A以及它们所有可能得切分点a中，选择基尼指数最小得特征机器对应的切分点作为最优特征与最优切分点。依据此从现结点生成俩子结点，将训练集依特征分配到两个子结点中去。

3）对两个子结点递归调用1）2），直至满足停止条件

4）生成CART决策树



## 决策树的剪枝

基本策略：预剪枝，后剪枝

### 预剪枝

（从上到下）

计算划分前与划分后精度，若划分前精度大于等于划分后精度，则进行剪枝

**预剪枝基于“贪心”本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险**

### 后剪枝

（从下到上）

计算剪枝前后精度，若剪枝后精度大于剪枝前精度，则进行剪枝

#### 极小化决策树整体损失函数

决策树学习的损失函数
$$
C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|
$$

$$
H_t(T)=-\sum_k\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}
$$

$$
|T|为叶节点的个数，H_t(T)为叶节点的经验熵，N_t为该叶节点的总样本点，N_{tk}是k类样本点的个数
$$

$$
C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_tlog\frac{N_{tk}}{N_t}
$$

$$
C_{\alpha}=C(T)+\alpha|T|
$$

alpha：较大时，选择较简单的模型；较小时选择较复杂的模型

1）计算每个结点的经验熵

2）递归从叶节点向上回缩，如果
$$
C_{\alpha}(T_A)<=C_{\alpha}(T_B)
$$
则进行剪枝

3）返回2）直至不能继续

## 连续与缺失值处理

### 连续属性离散化

基于划分点t可将D分为子集Dt-（在属性a熵取值不大于t的样本）和Dt+（在属性a熵取值大于t的样本）
$$
T_a=\{\frac{a^i+a^{i+1}}{2}|1<=i<=n-1\}
$$

$$
Gain(D,a)=\max_{t\in{T_a}}Ent(D)+\sum_{\lambda\in\{-,+\}}\frac{|D^v|}{|D|}Ent(D_t^{\lambda})
$$

1）从小到大排序

2）两两算均值

3）计算最大信息增益

### 缺失值处理



## 随机森林

随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树，而它的本质属于机器学习的一大分支——集成学习（Ensemble Learning）方法。

将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器，这就是随机森林bagging的思想



参考资料：

《机器学习》 周志华

《机器学习公式详解》 谢文睿 秦州

《统计学习方法》 李航

